---
title: 第三周 Classification
date: 2021-01-06 14:03:58
tags: 吴恩达机器学习
---

之前两周学习了线性回归，也就是每一个输入他都会给出一个输出，现在学习分类问题，就是会把输出分到几个大类里面（有限个outcomes），比如将邮件分为垃圾邮件与正常邮件等。

### 一. Classification and representation

#### 1. Classification
对于二分类问题来说，如果用线性回归来做，画一条直线，大于多少的为1，小于多少的为0，很多方面不妥，所以接下来会介绍二分类问题的具体解法。

#### 2. Hypothesis Representation
给出了H假想的公式，逻辑回归会给出：输入为这个的情况下，输出为1（或者0）的概率为多少。

#### 3. Decision boundary
如何二分类，找到一个H(公式)，H(公式)大于0.5时表示1，H(公式)小于0.5时表示2。而公式的划分条件就是大于0或者小于0，公式就是参数和变量组成的方程。
https://www.coursera.org/learn/machine-learning/supplement/N8qsm/decision-boundary

### 二. Logistic Regression Model

#### 1. Cost function
二分类问题的“公式”已经找到了，那么如何确定公式里面的参数呢，还是像往常一样先考虑Cost function，但是这个和之前线性回归的cost function就不一样了。
对于y=1和y=0两种情况J是不一样的，具体图像参照 https://www.coursera.org/learn/machine-learning/supplement/bgEt4/cost-function
如果判断错了那么cost将会无穷大
注意：cost function对于逻辑回归要保证J是“凸函数”

#### 2. Simplified Cost Function and Gradient Descent
我们之前发现cost function对于不同值的时候是不一样的，现在我们将两个cost function合并成为一个（让y=1-y表示另外一个），写出最终的cost function.
跟之前差不多，表示出Gradient Descent. 

#### 3. Advanced Optimization
这部分没太学明白，之后再看一遍

### 三. Multiclass classification
之前是二分类，那么如果多个分类怎么办呢，这里我们还是把它当成二分类问题，就是说把这个和其他所有分开，如果有k个类，那就做k次二分类。

### 四. Solving problem of overfitting(过拟合)

#### 1. The problem of overfitting
过拟合就是为了减少cost而绘制“分割线”，这样虽然减少了training set的cost但是新来的样本预测的就不好了。
有两个方式解决这个问题，要么手动减少特征，别啥都算进来，或者有一种选模型的算法（之后会讲）。要么用regularization方式，选择所有特征但是调整参数的值。

#### 2. Regularized cost function
我们通过shrink all parameters的方式：https://www.coursera.org/learn/machine-learning/supplement/1tJlY/cost-function

#### 3. Regularized Linear Regression

#### 4. Regularized Logistic Regression

