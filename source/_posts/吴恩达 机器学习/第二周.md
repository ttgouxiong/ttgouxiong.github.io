---
title: 第二周 Linear Regression with Multiple Variables
date: 2021-01-03 18:12:33
tags: 吴恩达机器学习
---

### 一. Multivariate Linear Regression

#### 1. Multiple features
之前一章节讲了只有一个变量影响价格，这一部分我们要讨论多个变量影响价格，所以会有多个参数，要注意如何表示不同的参数与变量以及他们的运算。

#### 2. Gradient descent for multiple variables
https://www.coursera.org/learn/machine-learning/supplement/aEN5G/gradient-descent-for-multiple-variables
和前面的章节差不多，这里参数给出了一个通用公式

#### 3. Gradient Descent in Practice I - Feature Scaling
假设有两个变量x1,x2，x1范围是1-2000，x2范围是1-5，这么大的范围差距会导致一个变量下降很快一个很慢，从而导致方方面面的问题，比如“下降路线很曲折”，所以要想办法减少这种情况的发生。一个是scale fitting,一个是mean normalization。具体方法参照：https://www.coursera.org/learn/machine-learning/supplement/CTA0D/gradient-descent-in-practice-i-feature-scaling

#### 4. Gradient Descent in Practice II - Learning Rate
通过图像判断是否正确进行了梯度下降，learning rate大了就不能保证每一次J都减小，或者压根不减小，Learning rate小了倒是能保证减小，但是速率可能太慢浪费时间和其他资源。

#### 5. Features and Polynomial Regression
刚才变量的系数为一，现在这里先考虑一个变量，次数不仅仅是一，还有0.5，2，3...

### 二. Computing Parameters Analytically

#### 1. Normal equation
这是另外一种方式，直接通过矩阵运算来求参数。下面是与梯度下降法的对比：

|Gradient descent|Normal equation|
|:-----:|:-----:|
|Need to choose alpha|No need to choose alpha|
|Needs many iterations|No need to iterate|
|O(k×n×n)|O(n×n×n),need to calculate inverse of ...|
|Works well when n is large|Slow if n is very large|

#### 2. Normal Equation Noninvertibility
这里介绍了当矩阵没有逆矩阵的时候的情况分析和处理办法
https://www.coursera.org/learn/machine-learning/supplement/66bi5/normal-equation-noninvertibility

