---
title: 第一周 Linear Regression with One Variable
date: 2021-01-02 18:29:46
tags: 吴恩达机器学习
---

寒假终于有大块的时间系统的自学机器学习了，以前看过一遍course吴恩达的机器学习课程但是时间安排过于零散，下学期UST也有统计机器学习的课程，所以打算这个寒假好好从头学一遍，做好笔记，当成一个大纲为之后的学习铺好路，那就开始吧~
课程链接：https://www.coursera.org/learn/machine-learning/home/welcome

### 一. 介绍
#### 1. 什么是机器学习？
Samuel 给出定义较为古老的，非正式的定义： "the field of study that gives computers the ability to learn without being explicitly programmed."

Tom Mitchell给了一个更为正式的定义: "A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E."

以下棋为例：
E：和很多棋手下棋的经验
T：下棋这项任务
P：下一场棋局胜利的概率

一般来说机器学习可以被分类为：监督学习和非监督学习

#### 2. 监督学习
在监督学习中，我们有一个数据集并且我们知道正确的输出是什么样的，我们觉得输入和输出之间有某种关系
监督学习被分为**回归**和**分类**两个方面。对于回归问题，我们尝试预测的结果是连续的输出，就是说我们想把输入和某种连续函数关系联系在一起。而对于分类问题我们试图把预测结果分到离散的输出，也就是说我们尝试把输入与离散的分类类别联系到一起。

例1：
对于一个房价市场，我们有不同大小的房子，我们想预测他们的房价。房价作为房子大小的一个连续的输出，所以这是一个回归问题。但是当我们将输出变为：价格高于定价还是低于定价？这就变成了一个分类问题，我们将房子依照价格分成了两类。

例2：
回归：给一张人的图片，根据图片预测人的年龄
分类：一个肿瘤患者，预测这个肿瘤是良性还是恶性

#### 3. 非监督学习
非监督学习允许我们在不清楚结果的情况下面对问题，我们在区分数据的结构的时候不需要知道变量的作用。

我们可以就数据中各个变量之间的关系来聚类分析整体的结构

对于非监督学习，对预测的结果没有反馈

例：
聚类：收集了1000000不同的基因，找到一种方式来根据其中变量（寿命，位置，地位）的相似性自动分类成不同的组。
非聚类："鸡尾酒会效应"，在混乱的声音中分类区分出个体的声音

### 二. Model and cost function（模型和损失函数）

#### 1. Model representation
这部分介绍了如何如何表示输入输出，模型是什么
https://www.coursera.org/learn/machine-learning/supplement/cRa2m/model-representation

#### 2. Cost Function
我们想用一条线来预测这些点的分布，这条线上点的值与真实的值有误差，所以用cost function来定量的描述这个误差（均方误差）

#### 3. Cost function intuition1
我们想找到最合适的那一条线，来尝试贯穿所有的点。如何找到最合适的那一条呢？在这里我们做一下简化，把常数变量去掉，变成最简单的一元函数。我们将不同取值的参数带进cost function得到最终的差值，再画出插值与参数的图像，图线的最低点就是我们想找的误差最小的，最合适的那一条线。

#### 4. Cost function intuition2
这里我们要考虑下之前省略掉的常数参数，所以现在需要考虑两个参数对与cost function的影响了，那如何像之前一样用画图方式表示出来呢，可以画三维图用深度来表示误差的大小，或者用等高线图来表示在二维的坐标系上。这样一来我们通过看图来找出参数取什么值的时候误差最小，这条线最好。但是问题来了，不可能每一次都画出图来看，有没有什么算法能够自动帮我们找出来最好的那条线呢，这也是下一部分要讲的内容。

### 三. Parameter learning

#### 1. Gradient descent(梯度下降)
这部分讲的是梯度下降算法如何帮助我们找到最好的参数。我们用之前提到的3D图来思考，x,y坐标就是两个参数，这个点的高度代表此时的cost，我们当然是想找到最低的点也就是cost最小，这个坐标就是最好的参数。这里具体的公式参照：https://www.coursera.org/learn/machine-learning/supplement/2GnUg/gradient-descent

#### 2. Gradient descent intuition
https://www.coursera.org/learn/machine-learning/supplement/QKEdR/gradient-descent-intuition
这部分解释了梯度下降法的原理，是如何不断“下降”的。以及对不同learning rate会出现的情况的讨论，还有learning rate固定的情况下结果还是会不断地converge

#### 3. Gradient Descent For Linear Regression

### 四. 线性代数基础

#### 1. Matrices and vectors
#### 2. Addidion and scalar multiplication
#### 3. Matrix-vector multiplication
#### 4. Matrix-matrix multiplication
#### 5. Matrix multiplication properties
#### 6. Inverse and transpose
