---
title: （三）分布式文件系统HDFS
date: 2020-12-24 20:52:13
tags: 大数据技术原理与应用
---

### 一. 分布式文件系统

#### 1. 计算机集群结构
分布式文件系统把文件存储到多个计算机节点上，成千上万计算机节点构成计算机集群
目前分布式文件系统所采用的计算机集群都是由普通硬件构成的，这就大大降低了硬件上的开销

#### 2. 分布式文件系统的结构
节点分为**主节点**也被称为**名称节点**，以及**从节点**被称为**数据节点**

### 二. HDFS简介
总体而言，HDFS要实现以下目标：
- 兼容廉价的硬件设备（以前都是高端设备一个小型机几十万，现在普通PC机都可以）
- 流数据读写（基于处理海量数据，批处理这样的需求而建的）
- 大数据集
- 简单的文件模型（为了高效的读写文件，允许追加数据不允许修改数据）
为了实现上述优良性能也使得自身具有一些应用局限性：
- 不适合低延迟数据访问（面对大量数据都是一次读取全部再筛选想要查找的数据，实时性较差，HBase支持随机读写实时性较好，后面会介绍）
- 无法高效存储大量小文件（名称节点记录这些文件存储信息，文件太多导致索引结构复杂）
- 不支持多用户写入以及任意修改文件

### 三. HDFS相关概念
#### 1. 块
HDFS默认一个块64MB，一个文件被分成很多块，以块作为存储单位，为了分担磁盘寻址开销，也就是在大量数据间分摊磁盘寻址的开销
块的大小远大于普通文件系统（几千字节），（需要找元数据目录，找数据节点，从数据节点取数据），块太小寻址开销太大。同时如果块太大影响MapReduce效果，并行度下降，所以块大小要适宜。
块概念好处：
- 支持大规模文件存储：大规模文件存不下，可以分拆成一个个块存到不同节点，不会受到单个节点的存储容量限制。
- 简化系统设计：块大小固定，文件除以这个大小很快可以算出大概需要多少块。
- 适合数据备份：每个块文件都可以冗余存储到多个节点上，大大提高了系统的容错性和可用性。

#### 2. 名称节点
主节点，整个HDFS管家，存储元数据，数据目录，记录了文件是啥，文件被分成多少块，块和文件怎么映射的，每个块被存放在哪个服务器上。
名称节点负责管理分布式文件系统的命名空间，保存了两个核心的数据结构，FsImage和EditLog。
- FsImage：用于维护文件系统树以及文件树中所有的文件和文件夹的元数据，通过FsImage可以知道整个文件系统目录是什么样子的。FsImage没有记录块存储在哪一个数据节点，而是名称节点中某一单独区域存储，当数据节点加入名称节点，向名称节点汇报，名称节点自己生成清单记录，是两者不断沟通来记录的。
- EditLog：记录整个运行过程中对数据进行了什么创建删除重命名操作。

名称节点启动：
![](/images/大数据概述/名称节点启动.jpg)
将FsImage文件中的内容（历史数据）全部加载到内存中，之后再执行EditLog文件中的各项操作，使得内存中的元数据和实际的同步，得到最新元数据，保留新版的FsImage(名称节点)，旧的可以删除，同时创建新的EditLog。为什么用FsImage还要用EditLog呢，原因就是FsImage对打的HDFS非常大，所有更新操作都在这个里面会导致系统运行缓慢，所以用小很多的EditLog来操作。但是随着时间推迟Editlog又会不断增大，怎么办呢？答案是用第二名称节点。
&nbsp; 
第二名称节点：HDFS架构中的一个组成部分，一方面解决EditLog不断增大问题，同时又是名称节点冷备份（HDFS1.0）
![](/images/大数据概述/第二名称节点.jpg)
随着EditLog不断增大，第二名称节点会定期与名称节点通信，在某个阶段让名称节点停止使用Editlog，名称节点会生成新的一个EditLog，之前的让第二名称节点取走，第二名称节点将FsImage与EditLog都下载到本地，本地做合并得到新的FsImage，发送给名称节点，名称节点将Editlog.new变成EditLog。通过这种方式实现EditLog增大时与FsImage的合并，同时又完成冷备份。

#### 3. 数据节点
具体存储数据的，每个数据节点数据被保存在本地的Linux文件系统当中。

### 四. HDFS体系结构

#### 1. HDFS体系结构概述
主从结构模型，存储数据都是名称节点调度，数据节点实际存储数据。

#### 2. HDFS命名空间管理
HDFS命名空间包含目录，文件和块。访问的话和正常文件系统没啥区别：/+目录名称

#### 3. 通信协议
所有的HDFS通信协议都是构建在TCP/IP协议基础之上的
![](/images/大数据概述/通信协议.jpg)

#### 4. HDFS体系结构的局限性
HDFS11.0只设置了一个名称节点，简化了设计但是带来一些明显局限性：
- 命名空间限制：名称节点保存至内存中，内存空间有上限，元数据存储有上限，导致存储文件个数有上限。
- 性能瓶颈：整个分布式文件系统的吞吐量，受限于单个名称节点的吞吐量。
- 隔离问题：只有一个名称节点，只有一个命名空间，因此无法对不同应用程序隔离。
- 集群的可用性：容易发生单点故障，会导致整个集群不可用。虽然之前提到第二名称节点，但是是冷备份，需要待一段时间恢复数据，才能再运行。HDFS2.0解决了这个问题，用两个名称节点分区处理，热备份。

### 五. HDFS存储原理

#### 1. 冗余数据保存
由于每一个机器都很便宜，容易出错，HDFS会对每一个数据进行冗余存储，一般默认保存3份。这样冗余保存有3个好处：
- **加快数据传输速度**：假设A，B，C三个用户同时访问同一数据，之前的情况一定有先后，现在有3个机器一样的数据可以并行处理。
- **容易检查数据错误**：有参照，容易检查。
- **保证数据可靠性**：一旦发生故障HDFS探测到错误，会对正确数据自动复制到设定的数量，保证数据的可靠。

#### 2. 数据存储策略
- 数据存放：
第一个数据：放置在上传文件的数据节点；如果是集群外提交，则随机挑选一台磁盘不太满，CPU不太繁忙的节点。
第二个数据：与第一个副本不同的机架的节点上
第三个数据：与第一个副本相同的其他节点上
更多副本：随机节点
- 数据读取：就近读取
HDFS提供API可以确定数据节点所属的机架ID，客户端也可以调用API获取自己所属的机架ID，先找同一机架的，快代价小，如果没有就随机。

#### 3. 数据错误与恢复
- 名称节点出错：系统暂停运行，利用第二名称节点冷备份恢复数据，再启动，HFDS(2.0不用停止系统)
- 数据节点出错：数据节点会定期向名称节点发送‘心跳信息’报告自身状态，若一段时间收不到信息则名称节点标记他为宕机，复制其他数据节点数据。HDFS和其他分布式文件系统最大区别就是可以调整冗余数据的位置，不仅是发生错误，发现机器负载不均匀时也可以调整位置。
- 数据出错：磁盘损坏等情况会发生数据错误，在文件被创建时客户端会生成校验码一起存储，读取数据时计算数据得出校验码与当初不一致说明出现错误

### 六. HDFS数据读写过程
这里因为要设计编程实践帮助理解，之后再补上。